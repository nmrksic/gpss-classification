\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\reset@newl@bel
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{duvenaud13}
\citation{duvenaud13,lloyd14}
\citation{gha12}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Bayesian Nonparametric Models}{3}{section.2.1}}
\citation{orbanzteh}
\citation{orbanzteh}
\citation{orbanzteh}
\citation{rasmussenGPs}
\citation{rasmussen06}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Gaussian Processes}{5}{section.2.2}}
\citation{rasmussen06,duvenaud13}
\citation{rasmussenGPs}
\citation{zoubinoccam}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces {Functions drawn from {{GP}} priors with {\textsc  {SE}} covariance functions. Lengthscales of 1.0 (left) and 5.0 (right) reflect the length of the trends the kernel is able to capture. }\relax }}{6}{figure.caption.4}}
\citation{rasmussen06}
\citation{rasmussenGPs}
\citation{rasmussen06}
\citation{rasmussen06}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Gaussian Process Regression}{7}{section.2.3}}
\citation{zoubinoccam,rasmussen06,rasmussenGPs}
\citation{rasmussenGPs}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Learning the Hyperparameters}{8}{subsection.2.3.1}}
\citation{rasmussen06}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Gaussian Process Classification}{9}{section.2.4}}
\citation{rasmussen06,gpcapprox,gpcinference}
\citation{gpcinference}
\citation{gpcapprox}
\citation{gpcinference}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Approximative Inference for {{GP}} Classification}{10}{subsection.2.4.1}}
\citation{gpcinference}
\citation{gpcapprox}
\citation{minka}
\citation{gpcapprox,gpcinference}
\citation{gpcapprox,gpcinference}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}The Laplace Approximation}{11}{subsection.2.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Expectation Propagation}{11}{subsection.2.4.3}}
\citation{christoudias2009bayesian}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Work}{13}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Kernel Learning}{13}{section.3.1}}
\citation{lawrence2005probabilistic}
\citation{salakhutdinov2008using}
\citation{duvenaud2011additive11}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Additive Gaussian Processes}{14}{section.3.2}}
\citation{duvenaud2011additive11}
\citation{duvenaud2011additive11}
\citation{duvenaud2011additive11}
\citation{duvenaud13}
\citation{lloyd14}
\citation{schwarz1978estimating}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Kernel structure discovery for GP regression}{16}{section.3.3}}
\citation{duvenaud13,lloyd14}
\citation{lloyd14}
\citation{duvenaud13}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Kernel Structure Discovery for GP Classification}{19}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Building the Models}{20}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Defining the Kernel Grammar}{20}{subsection.4.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Searching for the Model}{22}{subsection.4.1.2}}
\citation{lloyd14}
\citation{duvenaud13}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces {The search path taken when the algorithm is applied to the six dimensional Liver data set. At each stage, the search determines the kernel with the best BIC value (blue), and uses the expansion operators to generate the next set of kernels. The search successfully identifies a three-way interaction between dimensions 5, 3 and 4. Once the next best kernel (red) can not improve on the best BIC value, the search terminates. \nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {} }\relax }}{25}{figure.caption.9}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Choosing the Model}{26}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Approximative Inference}{26}{subsection.4.2.1}}
\citation{schwarz1978estimating}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Bayesian and Akaike Information Criteria}{27}{subsection.4.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}The Number of Effective Hyperparameters}{28}{subsection.4.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Cross-validated Training Accuracy}{29}{subsection.4.2.4}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces { The search log of the cross-validation guided procedure on the Pima dataset. \nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {} }\relax }}{30}{table.caption.11}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tbl:crossval1}{{4.1}{30}{{ The search log of the cross-validation guided procedure on the Pima dataset. ~~~~~~~~~~~~ }\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Experiments with Synthetic Data}{30}{section.4.3}}
\citation{elementsofstatlearning}
\newlabel{tbl:synthetic}{{\caption@xref {tbl:synthetic}{ on input line 3}}{31}{Experiments with Synthetic Data}{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces {Kernel structures discovered by the BIC light guided search procedure. The kernel used to generate the data is shown on the left. Kernel structures discovered when the signal to noise ratios were 1:1 and 10:1 are shown on the right.} \relax }}{31}{table.caption.12}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces {Test accuracies of the kernels constructed by the search compared to the accuracies of the kernel and the function used to generate the synthetic data sets.} \relax }}{32}{table.caption.13}}
\newlabel{tbl:synthetic2}{{4.3}{32}{{Test accuracies of the kernels constructed by the search compared to the accuracies of the kernel and the function used to generate the synthetic data sets.} \relax }{table.caption.13}{}}
\citation{randomforest}
\citation{duvenaud2011additive11}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Structure Discovery for Real-world Data Sets}{33}{section.4.4}}
\newlabel{tbl:datasets}{{\caption@xref {tbl:datasets}{ on input line 3}}{33}{Structure Discovery for Real-world Data Sets}{table.caption.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Data set sizes, numbers of dimensions and the average time that each flavour of the structure search took to determine the kernel structure to use for that data set.\relax }}{33}{table.caption.15}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces {Mean classification errors of the GP Structure Search (GPSS). }\relax }}{34}{table.caption.17}}
\newlabel{tbl:ClassificationErr1}{{4.5}{34}{{Mean classification errors of the GP Structure Search (GPSS). }\relax }{table.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Overfitting}{34}{subsection.4.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The level of overfitting at each stage of the search, averaged over all experiments across the four data sets. The box plots show the 25th and 75th data percentile.\relax }}{35}{figure.caption.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Overfitting using short lengthscales on the Liver data set. \relax }}{36}{figure.caption.20}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Train and test accuracies across 100 ten-fold cross validation experiments. \relax }}{37}{figure.caption.22}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Providing Interpretability}{37}{section.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Pima Indian Diabetes Dataset}{38}{subsection.4.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The posterior mean plots of the additive components of $\textsc  {SE}_2 \tmspace  -\thinmuskip {.1667em} \times \tmspace  -\thinmuskip {.1667em}\textsc  {SE}_7  \tmspace  +\thinmuskip {.1667em} + \tmspace  +\thinmuskip {.1667em}\textsc  {SE}_6  \tmspace  +\thinmuskip {.1667em} + \tmspace  +\thinmuskip {.1667em}\textsc  {SE}_8$. This kernel structure was identified in four out of ten cross validation experiments on the Pima data set. \relax }}{39}{figure.caption.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The posterior mean plots of the additive components of $\textsc  {SE}_2  \tmspace  +\thinmuskip {.1667em} + \tmspace  +\thinmuskip {.1667em}\textsc  {SE}_6 \times \textsc  {SE}_7  \tmspace  +\thinmuskip {.1667em} + \tmspace  +\thinmuskip {.1667em}\textsc  {SE}_8$. This kernel structure was identified in three out of ten cross validation experiments on the Pima data set.\relax }}{40}{figure.caption.24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Wisconsin Diagnostic Breast Cancer Dataset}{41}{subsection.4.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The posterior mean plots of the one-dimensional additive components of $\textsc  {SE}_1  \tmspace  +\thinmuskip {.1667em} + \tmspace  +\thinmuskip {.1667em}\textsc  {SE}_2  \tmspace  -\thinmuskip {.1667em} \times \tmspace  -\thinmuskip {.1667em}\textsc  {SE}_6  \tmspace  -\thinmuskip {.1667em} \times \tmspace  -\thinmuskip {.1667em}\textsc  {SE}_8  \tmspace  +\thinmuskip {.1667em} + \tmspace  +\thinmuskip {.1667em}\textsc  {SE}_5 $, identified in the Breast data set experiments.\relax }}{41}{figure.caption.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces The posterior mean plots of the two additive components in the kernel structure: $ \textsc  {SE}_2  \tmspace  -\thinmuskip {.1667em} \times \tmspace  -\thinmuskip {.1667em}\textsc  {SE}_6  \tmspace  +\thinmuskip {.1667em} + \tmspace  +\thinmuskip {.1667em}\textsc  {SE}_7  \tmspace  -\thinmuskip {.1667em} \times \tmspace  -\thinmuskip {.1667em}\textsc  {SE}_1 $, identified in the Breast data set experiments.\relax }}{42}{figure.caption.26}}
\citation{kim08}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Towards the Automated Statistician}{43}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Robust Gaussian Process Classification}{43}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Modifying the Likelihood Function}{44}{subsection.5.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Mixtures of likelihood functions for different levels of salt and pepper noise.\relax }}{45}{figure.caption.27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Evaluating the Likelihood Mixture Model}{45}{subsection.5.1.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Comparison of the kernels discovered by the new (likMix) and the old (likErf) structure discovery procedures. The kernel structure used to generate the synthetic data set was $\textsc  {SE}_1  \tmspace  +\thinmuskip {.1667em} + \tmspace  +\thinmuskip {.1667em}{\textsc  {SE}_3  \tmspace  -\thinmuskip {.1667em} \times \tmspace  -\thinmuskip {.1667em}\textsc  {SE}_5  \tmspace  -\thinmuskip {.1667em} \times \tmspace  -\thinmuskip {.1667em}\textsc  {SE}_7}  \tmspace  +\thinmuskip {.1667em} + \tmspace  +\thinmuskip {.1667em}{\textsc  {SE}_9}$ (ten-dimensional data). \relax }}{46}{table.caption.29}}
\newlabel{tbl:syntheticlik7}{{5.1}{46}{Comparison of the kernels discovered by the new (likMix) and the old (likErf) structure discovery procedures. The kernel structure used to generate the synthetic data set was $\SE _1 \kernplus {\SE _3 \kerntimes \SE _5 \kerntimes \SE _7} \kernplus {\SE _9}$ (ten-dimensional data). \relax }{table.caption.29}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces {\relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Comparison of the likMix models' performance to that of the previous models. }\relax }}{46}{table.caption.31}}
\newlabel{tbl:Classification Percent Error}{{5.2}{46}{{\small Comparison of the likMix models' performance to that of the previous models. }\relax }{table.caption.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Bayesian Model Averaging}{47}{section.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces {Instead of the kernel structure with the best BIC value, the BMA procedure predicts using a weighted average of predictions from all kernels in the search layers one step before, in and after the layer where the best kernel structure is.}\relax }}{48}{figure.caption.32}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces { Bayesian Model Averaging improves predictive power on Breast and Liver. }\relax }}{49}{table.caption.35}}
\newlabel{tbl:BMAliver}{{5.3}{49}{{ Bayesian Model Averaging improves predictive power on Breast and Liver. }\relax }{table.caption.35}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces BMA fails to improve on Pima and completely ruins the performance on Heart.\relax }}{50}{table.caption.36}}
\citation{duvenaud13}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{51}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{lloyd14}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Further Work}{52}{section.6.1}}
\citation{gha12}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Bayesian Inference}{53}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{gha12}
\citation{zoubinoccam,rasmussen06}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}The Performance of the Likelihood Mixture Model}{57}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{tbl:syntheticLik1}{{\caption@xref {tbl:syntheticLik1}{ on input line 28}}{58}{Generating the Synthetic Data Sets}{table.caption.41}{}}
\newlabel{tbl:syntheticlik2}{{\caption@xref {tbl:syntheticlik2}{ on input line 70}}{58}{Generating the Synthetic Data Sets}{table.caption.42}{}}
\newlabel{tbl:synthetic3}{{\caption@xref {tbl:synthetic3}{ on input line 119}}{59}{Generating the Synthetic Data Sets}{table.caption.43}{}}
\newlabel{tbl:syntheticLik4}{{\caption@xref {tbl:syntheticLik4}{ on input line 156}}{59}{Generating the Synthetic Data Sets}{table.caption.44}{}}
\newlabel{tbl:syntheticlik5}{{\caption@xref {tbl:syntheticlik5}{ on input line 202}}{60}{Generating the Synthetic Data Sets}{table.caption.45}{}}
\newlabel{tbl:syntheticlik6}{{\caption@xref {tbl:syntheticlik6}{ on input line 244}}{60}{Generating the Synthetic Data Sets}{table.caption.46}{}}
\newlabel{tbl:syntheticlik7}{{\caption@xref {tbl:syntheticlik7}{ on input line 293}}{61}{Generating the Synthetic Data Sets}{table.caption.47}{}}
\newlabel{tbl:syntheticlik8}{{\caption@xref {tbl:syntheticlik8}{ on input line 338}}{61}{Generating the Synthetic Data Sets}{table.caption.48}{}}
\bibstyle{unsrt}
\bibdata{refs}
\bibcite{duvenaud13}{{1}{}{{}}{{}}}
\bibcite{lloyd14}{{2}{}{{}}{{}}}
\bibcite{gha12}{{3}{}{{}}{{}}}
\bibcite{orbanzteh}{{4}{}{{}}{{}}}
\bibcite{rasmussenGPs}{{5}{}{{}}{{}}}
\bibcite{rasmussen06}{{6}{}{{}}{{}}}
\bibcite{zoubinoccam}{{7}{}{{}}{{}}}
\bibcite{gpcapprox}{{8}{}{{}}{{}}}
\bibcite{gpcinference}{{9}{}{{}}{{}}}
\bibcite{minka}{{10}{}{{}}{{}}}
\bibcite{christoudias2009bayesian}{{11}{}{{}}{{}}}
\bibcite{lawrence2005probabilistic}{{12}{}{{}}{{}}}
\bibcite{salakhutdinov2008using}{{13}{}{{}}{{}}}
\bibcite{duvenaud2011additive11}{{14}{}{{}}{{}}}
\bibcite{schwarz1978estimating}{{15}{}{{}}{{}}}
\bibcite{elementsofstatlearning}{{16}{}{{}}{{}}}
\bibcite{randomforest}{{17}{}{{}}{{}}}
\bibcite{kim08}{{18}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
