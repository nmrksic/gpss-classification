\documentclass[twoside]{article}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{graphicx}
\usepackage{preamble}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{color}
\usepackage{wasysym}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfsubject={},
    pdfkeywords={},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}
    
\input{include/commenting.tex}
\usepackage{format/icml2013}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!
    
    
% If your paper is accepted, change the options for the package
% aistats2e as follows:
%
%\usepackage[accepted]{aistats2e}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.


\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\onecolumn

\icmltitlerunning{Structure Search Supplementary Material}

\icmltitle{Supplementary Material for
\\
Kernel Structure Discovery in Gaussian Process Models}

%\icmlauthor{Anonymous Author 1}
%\icmladdress{Anonymous Institution}

%\subsection{Real datasets}

\appendix



\section{Derivation of Component Marginal Variance}

Let us assume that our function $\vf$ is a sum of two functions, $\vf_1$ and $\vf_2$, where $\vf = \vf_1 + \vf_2$.  If $\vf_1$ and $\vf_2$ are a priori independent, and $\vf_1 \sim \gp( \vmu_1, k_1)$ and $\vf_2 \sim \gp( \vmu_2, k_2)$, then
\begin{align}
\left[ \begin{array}{c} \vf_1 \\ \vf_1^\star \\ \vf_2 \\ \vf_2^\star \\ \vf \\ \vf^\star \end{array} \right]
\sim
\Nt{\left[ \begin{array}{c} \vmu_1 \\ \vmu_1^\star \\ \vmu_2 \\ \vmu_2^\star \\ \vmu_1 + \vmu_2 \\ \vmu_1^\star + \vmu_2^\star \end{array} \right]
}
{\left[ \begin{array}{cccccc} 
\vk_1 & \vk_1^\star & 0 & 0 & \vk_1 & \vk_1^\star \\ 
\vk_1^\star & \vk_1^{\star\star} & 0 & 0 & \vk_1^\star & \vk_1^{\star\star} \\
0 & 0 & \vk_2 & \vk_2^\star & \vk_2 & \vk_2^\star \\ 
0 & 0 & \vk_2^\star & \vk_2^{\star\star} & \vk_2^\star & \vk_2^{\star\star} \\
\vk_1 & \vk_1^\star & \vk_2 & \vk_2^\star & \vk_1 + \vk_2 & \vk_1^\star + \vk_2^\star \\ 
\vk_1^\star & \vk_1^{\star\star}  & \vk_2^\star & \vk_2^{\star\star}  & \vk_1^\star + \vk_2^\star & \vk_1^{\star\star} + \vk_2^{\star\star}\\
\end{array} \right]
}
\end{align}
where $\vk_1 = k_1( \vX, \vX )$ and $\vk_1^\star = k_1( \vX^\star, \vX )$. 

By the formula for Gaussian conditionals:
\begin{align}
\vx_A | \vx B \sim \Nt{\vmu_A + \vSigma_{AB} \vSigma_{BB}\inv \left( \vx_B - \vmu_B \right) }
{\vSigma_{AA} - \vSigma_{AB} \vSigma_{BB}\inv \vSigma_{BA} },
\end{align}
we get that the conditional variance of a Gaussian conditioned on its sum with another Gaussian is given by
\begin{align}
\vf_1^\star | \vf \sim \Nt{\vmu_1^\star + \vk_1^{\star\tra} (\vK_1 + \vK_2)\inv \left( \vf - \vmu_1 - \vmu_2 \right) }
{\vk_1^{\star\star} - \vk_1^{\star\tra} (\vK_1 + \vK_2)\inv \vk_1^\star }.
\end{align}

The covariance between the two components, conditioned on their sum is given by:
\begin{align}
\cov(\vf_1^\star, \vf_2^\star) | \vf = - \vk_1^{\star\tra} (\vK_1 + \vK_2)\inv \vk_2^\star
\end{align}

These formulae express the posterior model uncertainty about different components of the signal, integrating over the possible configurations of the other components.


\section{Details of Search Algorithm}

Formally\fTBD{Maybe Roger is well placed to write this?}, we start with a collection of base kernels applied to each input dimension individually; $\kernel_i$ denotes a kernel applied to input dimension $i$.
We denote sums and products of kernels as $\SumKernel(\expression,\ldots)$ and $\ProductKernel(\expression,\ldots)$ where $\expression$ represents an arbitrary kernel expression.
We then generate new expressions by repeatedly applying the following production rules to base kernels, sum kernels and product kernel within an expression.
\begin{center}
\begin{tabular}{rccc}
\textrm{Replacement} & $\kernel_i$ & $\to$ & $\kernel'_i$\\% & $\forall\, \kernel' $\\
\textrm{Addition} & $\kernel_i$ & $\to$ & $\kernel_i + \kernel'_j$\\% & $\forall\, j,\kernel' $\\
& $\SumKernel(e,\ldots)$ & $\to$ & $\SumKernel(e,\ldots,\kernel'_j)$\\% & $\forall\, j,\kernel' $\\
& $\ProductKernel(e,\ldots)$ & $\to$ & $\SumKernel(\ProductKernel(e,\ldots),\kernel'_j)$\\% & $\forall\, j,\kernel' $\\
\textrm{Multiplication} & $\kernel_i$ &  $\to$ & $\kernel_i \times \kernel'_j$\\% & $\forall\, j,\kernel'$\\
& $\SumKernel(e,\ldots)$ & $\to$ & $\ProductKernel(\SumKernel(e,\ldots),\kernel'_j)$\\% & $\forall\, j,\kernel' $\\
& $\ProductKernel(e,\ldots)$ & $\to$ & $\ProductKernel(e,\ldots,\kernel'_j)$\\% & $\forall\, j,\kernel' $\\
\end{tabular}
\end{center}
For example, applying the multiplication production rule to the sum in the expression $\SumKernel(\kernel_\textrm{SE},\kernel_\textrm{SE})$ could result in the new expression $\ProductKernel(\SumKernel(\kernel_\textrm{SE},\kernel_\textrm{SE}),\kernel_\textrm{PE})$ \ie a kernel representing a smooth function with two characteristic scales of variation is transformed into a kernel representing a locally periodic function with two scales of variation.

\paragraph{Remark on notation} The $\SumKernel,\ProductKernel$ notation is useful algorithmically and to describe the production rules but elsewhere we will use more conventional algebraic notation \ie the two kernels above would be written as $\kernel_\textrm{SE} + \kernel_\textrm{SE}$ and ${(\kernel_\textrm{SE} + \kernel_\textrm{SE}) \times \kernel_\textrm{PE}}$.


%\section{Decompositions}

%\begin{figure}
%\includegraphics[width=10cm]{../figures/decomposition/01-airline/01-airline_all.pdf}
%\caption{The complete posterior on the Airline dataset.}
%\label{fig:mauna_all}
%\end{figure}

%\newcommand{\fw}{8cm}
%\begin{figure*}
%\centering
%\begin{tabular}{cc}
% \includegraphics[width=\fw]{../figures/decomposition/01-airline/01-airline_7.pdf} &  \includegraphics[width=\fw]{../figures/decomposition/01-airline/01-airline_8.pdf} \\
%  \includegraphics[width=\fw]{../figures/decomposition/01-airline/01-airline_5.pdf} &  \includegraphics[width=\fw]{../figures/decomposition/01-airline/01-airline_6.pdf} \\
%   \includegraphics[width=\fw]{../figures/decomposition/01-airline/01-airline_3.pdf} &  \includegraphics[width=\fw]{../figures/decomposition/01-airline/01-airline_4.pdf} \\
%    \includegraphics[width=\fw]{../figures/decomposition/01-airline/01-airline_1.pdf} &  \includegraphics[width=\fw]{../figures/decomposition/01-airline/01-airline_2.pdf}
%\end{tabular}
%\caption{Automatic decomposition of airline data.
%}
%\label{fig:kernels}
%\end{figure*}


\section{Complete listings of chosen kernels}

Below, we show the best kernel structure found for each fold of the datasets used in the results tables.

%\input{"tables/kernels.tex"}
%\input{"tables/kernels2.tex"}
%\input{"tables/kernels3.tex"}
\input{"tables/kernels4.tex"}
\input{"tables/kernels5.tex"}

\bibliographystyle{format/icml2013}
\bibliography{gpss}

\end{document}


% Extra stuff



\section{James' Derivation of the Posterior Marginal Conditional Variance}
\begin{lem}
\label{lem:cond}
If
\begin{eqnarray}
x & \dist & \Normal(\mu, \Lambda^{-1}) \\
y \given x & \dist & \Normal(Ax + b, L^{-1})
\end{eqnarray}
then
\begin{eqnarray}
y & \dist & \Normal(A\mu + b, L^{-1} + A\Lambda^{-1}A') \\
x \given y & \dist & \Normal(\Sigma(A'L(y-b) + \Lambda\mu, \Sigma)
\end{eqnarray}
where
\begin{equation}
\Sigma = (\Lambda + A'LA)^{-1}.
\end{equation}
\end{lem}
\begin{prop}
If
\begin{eqnarray}
f_1 & \dist & \Normal(0, K_1) \\
f \given f_1 & \dist & \Normal(f_1, K_2) \\
f_1^* \given f_1 & \dist & \Normal(k_1^*K_1^{-1}f_1, K_1^{**} - k_1^*K_1^{-1}k_1^{*t})
\end{eqnarray}
then
\begin{eqnarray}
f_1 \given f & \dist & \Normal(\Sigma K_2^{-1}f, \Sigma) \\
f_1^* \given f & \dist & \Normal(k_1^*\Lambda f, K_1^{**} - k_1^*\Delta k_1^{*t})
\end{eqnarray}
where
\begin{eqnarray}
\Sigma & = & (K_1^{-1} + K_2^{-1})^{-1} \\
\Lambda & = & (K_1 + K_2)^{-1} \\
\Delta & = & K_1^{-1} - (K_1 + K_1K_2^{-1}K_1)^{-1}
\end{eqnarray}
\end{prop}

\begin{proof}
We first apply the second half of lemma~\ref{lem:cond} with $x=f_1$, $y=f$, $\mu=b=0$, $A=I$, $\Lambda^{-1} = K_1$ and $L^{-1} = K_2$ yielding
\begin{eqnarray}
f_1 \given f & \dist & \Normal(\Sigma K_2^{-1}f, \Sigma).
\end{eqnarray}
Since $f_1^*$ is independent of $f$ given $f_1$ we have $p(f_1^* \given f) = \int p(f_1^* \given f_1) p(f_1 \given f) \textrm{d}f_1$\PROBLEM{I think this is true - think graphical models} i.e.~we condition $f_1^*$ on the posterior of $f_1$.
We apply the first half of lemma~\ref{lem:cond} with $x=(f_1\given f)$, $y=f_1^*$, $\mu=\Sigma K_2^{-1}f$, $b=0$, $A = k_1^*K_1^{-1}$, $\Lambda^{-1}=\Sigma$ and $L^{-1} = K_1^{**} - k_1^*K_1^{-1}k_1^{*t}$.
Then for the mean of $f_1^* \given f$ we get the expression
\begin{eqnarray}
& k_1^*K_1^{-1} \Sigma K_2^{-1}f \\
= & k_1^*K_1^{-1} (K_1^{-1} + K_2^{-1})^{-1} K_2^{-1}f \\
= & k_1^*(K_1 + K_2)^{-1}f \\
= & k_1^*\Lambda f
\end{eqnarray}
and for the variance we obtain
\begin{eqnarray}
& K_1^{**} - k_1^*K_1^{-1}k_1^{*t} + k_1^*K_1^{-1} \Sigma K_1^{-1}k_1^{*t} \\
= & K_1^{**} - k_1^*(K_1^{-1} - (K_1 + K_1K_2^{-1}K_1)^{-1})k_1^{*t} \\
= & K_1^{**} - k_1^*\Delta k_1^{*t}
\end{eqnarray}
\end{proof}






\section{Load forecasting}

Similar to the above, we apply the methodology to another highly structured time series, but with additional input data.
We analyse hourly load data for a US utility; this data was recently the focus of a data mining competition; the load forecasting track of GEFCom2012\footnotemark.
The data consists of hourly load measurements of a US utility over several years, split into 20 geographical zones and 11 temperature time series.
The relationships (geographical or otherwise) between the zones and the temperature stations were unknown for the purposes of the competition.

\footnotetext{\texttt{http://www.gefcom.org/}}
\begin{figure}
\includegraphics[width=0.5\columnwidth]{../figures/gef_load_z01_t09_500}
\caption{Zone 1 and temperature station 9 from GEFCom2012 load forecasting data.}
\label{fig:gef_z01_t09}
\end{figure}

Figure~\ref{fig:gef_z01_t09} shows the first 500 data points (approx.~20 days) of zone 1 together with temperature station 9 (all data has been standardised to have zero mean and unit standard deviation).
The plot shows that the load data follows a smooth trend with near periodic deviations.
The overall trend is somehow related to the temperature and some of the spikes in the load data appear to be related to spikes in temperature.

We applied our kernel selection methodology to this data set (using all temperature stations).
The kernels discovered at subsequent levels of the search were:
\begin{enumerate}
\item \texttt{RQ\_t(-1.8,  0.0, -1.0)}
\item \texttt{RQ\_t(-1.4,  0.0, -2.1) + PE\_t(-0.7,  0.0, -0.8)}
\item \texttt{RQ\_t( 0.3, -0.1, -1.9) * ( PE\_t(-0.9,  0.0, -1.0) + RQ\_t(-0.6,  0.0, -2.2) )}
\item \texttt{RQ\_t( 0.3, -0.1, -2.1) * SE\_T9( 1.2,  0.0) * ( PE\_t(-0.8,  0.0, -0.9) + RQ\_t(-0.5, -0.1, -2.2) )}
\end{enumerate}
The learnt kernels can be interpreted as follows.
The simplest model explains the data as a smooth function of time, with a short lengthscale to account for all of the variation.
The next model explains the data as a sum of a smooth component and periodic component with a period of one day; the lengthscale of the smooth component has increased since some of the fine detail can be explained by the periodic component.
The next model also splits the function into smooth and periodic components, but the periodic component is multiplied by a rational quadratic kernel which allows it to vary smoothly through time; again the lengthscale of the smooth component has increased since the fine detail has been better explained by the periodic component.
Finally, the model introduces a temperature variable to further explain the observed data.

One can visualise how the different components of the kernel capture the structure of the data.
Figure~\ref{fig:gef_z01_two_means} shows the components of the Gaussian process posterior mean corresponding to the additive components of the kernel function (after expanding brackets and grouping product terms).

\begin{figure}
\includegraphics[width=0.5\columnwidth]{../figures/gef_load_z01_500_posteriors}
\caption{Zone 1 from GEFCom 2012 separated into smooth and periodic component}
\label{fig:gef_z01_two_means}
\end{figure}

\TBD{Not sure the below is going to work.}
These posterior means have been evaluated at a particular 1 dimensional trace through time and temperature.
We can however examine the posterior mean of the Gaussian process at other points in time and temperature space to better understand the interaction.
\TBD{If we plot the smooth component, it would be super awesome if we could see the weekend or other interpretable blips in time, whilst most of the variation in the trend might be explained by temperature. Kernel might not be complicated enough to really show this though \frownie.}

\TBD{What else can we say about this data? Numerical results might take a while to produce.}


